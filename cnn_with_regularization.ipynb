{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CNN with Regularization( Fashion-mnist)\n#### Design and Implementation of the a CNN model (with 4+ convolutional layers)\n- While Recording the Training & Test accuracy of the following\n  1. Base Model\n  2. L1 Regularization\n  3. L2 Regularization\n  4. Dropout\n  5. L2 (or L1) + Dropout","metadata":{}},{"cell_type":"markdown","source":"### Step 0: Setup and Imports","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T13:52:31.402165Z","iopub.execute_input":"2025-10-03T13:52:31.402453Z","iopub.status.idle":"2025-10-03T13:52:31.406717Z","shell.execute_reply.started":"2025-10-03T13:52:31.402430Z","shell.execute_reply":"2025-10-03T13:52:31.406024Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Step 1: Load and Preprocess Fashion Mnist","metadata":{}},{"cell_type":"code","source":"(x_train_fm, y_train_fm),(x_test_fm, y_test_fm) = tf.keras.datasets.fashion_mnist.load_data()\n\n# Normalize to [0,1]\nx_train = np.expand_dims(x_train_fm, -1,).astype('float32')/255.0\nx_test = np.expand_dims(x_test_fm, -1).astype('float32')/255.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T13:52:32.694866Z","iopub.execute_input":"2025-10-03T13:52:32.695125Z","iopub.status.idle":"2025-10-03T13:52:33.206430Z","shell.execute_reply.started":"2025-10-03T13:52:32.695108Z","shell.execute_reply":"2025-10-03T13:52:33.205791Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Step 2: Define Models\n* Base Model Function\n* Variants with Regularization","metadata":{}},{"cell_type":"code","source":"def build_base_model(input_shape=(28,28,1), num_classes=10):\n    model = tf.keras.Sequential([\n        # Layer 1\n        tf.keras.layers.Conv2D(32, (3,3), activation='relu',padding='same', input_shape=input_shape),\n        tf.keras.layers.MaxPooling2D(2,2),\n\n        # Layer 2\n        tf.keras.layers.Conv2D(64, (3,3), activation='relu',padding='same'),\n        tf.keras.layers.MaxPooling2D(2,2),\n\n        # Layer 3\n        tf.keras.layers.Conv2D(128, (3,3), activation='relu',padding='same'),\n        tf.keras.layers.MaxPooling2D(2,2),\n\n        # Layer 4\n        tf.keras.layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n        tf.keras.layers.Flatten(),\n\n        # Dense layer\n        tf.keras.layers.Dense(256, activation='relu'),\n        tf.keras.layers.Dense(num_classes, activation='softmax')\n    ])\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T14:15:07.562366Z","iopub.execute_input":"2025-10-03T14:15:07.562669Z","iopub.status.idle":"2025-10-03T14:15:07.568364Z","shell.execute_reply.started":"2025-10-03T14:15:07.562648Z","shell.execute_reply":"2025-10-03T14:15:07.567802Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"#### Step 2b: Variants with Regularization","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import regularizers\n\n# a L1\ndef build_l1_model(input_shape=(28,28, 1), num_classes=10):\n    return tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', input_shape=input_shape,kernel_regularizer=regularizers.l1(.001)),\n        tf.keras.layers.MaxPooling2D(2,2),\n\n        tf.keras.layers.Conv2D(64, 3, activation='relu',padding='same', kernel_regularizer=regularizers.l1(.001)),\n        tf.keras.layers.MaxPooling2D(2,2),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1(.001)),\n        tf.keras.layers.Dense(num_classes, activation='softmax')\n    ])\n\n# b L2\ndef build_l2_model(input_shape=(28,28, 1), num_classes=10):\n    return tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', input_shape=input_shape,kernel_regularizer=regularizers.l2(.001)),\n        tf.keras.layers.MaxPooling2D(2,2),\n\n        tf.keras.layers.Conv2D(64, 3, activation='relu',padding='same', kernel_regularizer=regularizers.l2(.001)),\n        tf.keras.layers.MaxPooling2D(2,2),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(.001)),\n        tf.keras.layers.Dense(num_classes, activation='softmax')\n    ])\n\n# C Dropout\ndef build_dropout_model(input_shape=(28,28,1), num_classes=10):\n    return tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\", input_shape=input_shape),\n        tf.keras.layers.MaxPooling2D(2,2),\n        tf.keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\"),\n        tf.keras.layers.MaxPooling2D(2,2),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation=\"relu\"),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n    ])\n\n# (d) L2 + Dropout\ndef build_l2_dropout_model(input_shape=(28,28,1), num_classes=10):\n    return tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, 3, activation=\"relu\", padding=\"same\", input_shape=input_shape,\n                               kernel_regularizer=regularizers.l2(0.001)),\n        tf.keras.layers.MaxPooling2D(2,2),\n        tf.keras.layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\", kernel_regularizer=regularizers.l2(0.001)),\n        tf.keras.layers.MaxPooling2D(2,2),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T14:19:12.348089Z","iopub.execute_input":"2025-10-03T14:19:12.348737Z","iopub.status.idle":"2025-10-03T14:19:12.361358Z","shell.execute_reply.started":"2025-10-03T14:19:12.348707Z","shell.execute_reply":"2025-10-03T14:19:12.360803Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"### Step 3: Training Utility","metadata":{}},{"cell_type":"code","source":"def train_and_evaluate(model, x_train, y_train, x_test, y_test, epochs=10):\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n    history = model.fit(x_train, y_train, epochs=epochs, batch_size=128, validation_split=.1,verbose=2)\n    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n    print(f'Test Accuracy: {test_acc:.4}')\n    return history, test_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T14:19:14.454517Z","iopub.execute_input":"2025-10-03T14:19:14.455204Z","iopub.status.idle":"2025-10-03T14:19:14.459386Z","shell.execute_reply.started":"2025-10-03T14:19:14.455176Z","shell.execute_reply":"2025-10-03T14:19:14.458801Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"### Step 4: Run Experiments","metadata":{}},{"cell_type":"code","source":"# a. Base\nx_train, x_test, y_train, y_test = x_train_fm, x_test_fm, y_train_fm, y_test_fm\nbase_history, base_acc = train_and_evaluate(build_base_model(), x_train, y_train, x_test, y_test)\n\n# b. L1\nl1_history, l1_acc = train_and_evaluate(build_l1_model(), x_train, y_train, x_test, y_test)\n\n# c. L2\nl2_history, l2_acc = train_and_evaluate(build_l2_model(), x_train, y_train, x_test, y_test)\n\n# d. Dropout\ndrop_history, drop_acc = train_and_evaluate(build_dropout_model(), x_train, y_train, x_test, y_test)\n\n# e. L2 + Dropout\ncombo_history, combo_acc = train_and_evaluate(build_l2_dropout_model(), x_train, y_train, x_test, y_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T14:19:17.403550Z","iopub.execute_input":"2025-10-03T14:19:17.404217Z","iopub.status.idle":"2025-10-03T14:21:25.017753Z","shell.execute_reply.started":"2025-10-03T14:19:17.404182Z","shell.execute_reply":"2025-10-03T14:21:25.017004Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n422/422 - 7s - 17ms/step - accuracy: 0.8240 - loss: 0.6873 - val_accuracy: 0.8837 - val_loss: 0.3181\nEpoch 2/10\n422/422 - 3s - 6ms/step - accuracy: 0.8939 - loss: 0.2895 - val_accuracy: 0.9038 - val_loss: 0.2738\nEpoch 3/10\n422/422 - 2s - 6ms/step - accuracy: 0.9089 - loss: 0.2447 - val_accuracy: 0.9048 - val_loss: 0.2722\nEpoch 4/10\n422/422 - 2s - 6ms/step - accuracy: 0.9197 - loss: 0.2172 - val_accuracy: 0.9073 - val_loss: 0.2500\nEpoch 5/10\n422/422 - 2s - 6ms/step - accuracy: 0.9284 - loss: 0.1909 - val_accuracy: 0.9058 - val_loss: 0.2550\nEpoch 6/10\n422/422 - 2s - 6ms/step - accuracy: 0.9353 - loss: 0.1721 - val_accuracy: 0.9138 - val_loss: 0.2565\nEpoch 7/10\n422/422 - 2s - 6ms/step - accuracy: 0.9384 - loss: 0.1639 - val_accuracy: 0.9142 - val_loss: 0.2528\nEpoch 8/10\n422/422 - 2s - 6ms/step - accuracy: 0.9461 - loss: 0.1440 - val_accuracy: 0.9128 - val_loss: 0.2539\nEpoch 9/10\n422/422 - 2s - 6ms/step - accuracy: 0.9511 - loss: 0.1315 - val_accuracy: 0.9123 - val_loss: 0.2706\nEpoch 10/10\n422/422 - 2s - 6ms/step - accuracy: 0.9508 - loss: 0.1284 - val_accuracy: 0.9085 - val_loss: 0.3135\nTest Accuracy: 0.91\nEpoch 1/10\n422/422 - 7s - 18ms/step - accuracy: 0.8161 - loss: 6.7403 - val_accuracy: 0.8665 - val_loss: 4.1011\nEpoch 2/10\n422/422 - 2s - 4ms/step - accuracy: 0.8755 - loss: 3.2371 - val_accuracy: 0.8757 - val_loss: 2.5375\nEpoch 3/10\n422/422 - 2s - 4ms/step - accuracy: 0.8805 - loss: 2.0778 - val_accuracy: 0.8703 - val_loss: 1.7028\nEpoch 4/10\n422/422 - 2s - 4ms/step - accuracy: 0.8869 - loss: 1.4196 - val_accuracy: 0.8868 - val_loss: 1.1800\nEpoch 5/10\n422/422 - 2s - 4ms/step - accuracy: 0.8890 - loss: 1.0329 - val_accuracy: 0.8908 - val_loss: 0.9034\nEpoch 6/10\n422/422 - 2s - 4ms/step - accuracy: 0.8904 - loss: 0.8142 - val_accuracy: 0.8887 - val_loss: 0.7511\nEpoch 7/10\n422/422 - 2s - 4ms/step - accuracy: 0.8926 - loss: 0.6854 - val_accuracy: 0.8947 - val_loss: 0.6407\nEpoch 8/10\n422/422 - 2s - 4ms/step - accuracy: 0.8949 - loss: 0.6030 - val_accuracy: 0.8912 - val_loss: 0.6019\nEpoch 9/10\n422/422 - 2s - 4ms/step - accuracy: 0.8950 - loss: 0.5557 - val_accuracy: 0.8970 - val_loss: 0.5485\nEpoch 10/10\n422/422 - 2s - 4ms/step - accuracy: 0.8954 - loss: 0.5203 - val_accuracy: 0.8785 - val_loss: 0.5736\nTest Accuracy: 0.8714\nEpoch 1/10\n422/422 - 7s - 16ms/step - accuracy: 0.8257 - loss: 1.2413 - val_accuracy: 0.8768 - val_loss: 0.5792\nEpoch 2/10\n422/422 - 2s - 4ms/step - accuracy: 0.8913 - loss: 0.5180 - val_accuracy: 0.8892 - val_loss: 0.5007\nEpoch 3/10\n422/422 - 2s - 4ms/step - accuracy: 0.9088 - loss: 0.4465 - val_accuracy: 0.8988 - val_loss: 0.4611\nEpoch 4/10\n422/422 - 2s - 4ms/step - accuracy: 0.9173 - loss: 0.4006 - val_accuracy: 0.9035 - val_loss: 0.4281\nEpoch 5/10\n422/422 - 2s - 4ms/step - accuracy: 0.9234 - loss: 0.3658 - val_accuracy: 0.9070 - val_loss: 0.4129\nEpoch 6/10\n422/422 - 2s - 4ms/step - accuracy: 0.9268 - loss: 0.3378 - val_accuracy: 0.9092 - val_loss: 0.4080\nEpoch 7/10\n422/422 - 2s - 4ms/step - accuracy: 0.9319 - loss: 0.3169 - val_accuracy: 0.9122 - val_loss: 0.3788\nEpoch 8/10\n422/422 - 2s - 4ms/step - accuracy: 0.9364 - loss: 0.2945 - val_accuracy: 0.9047 - val_loss: 0.3869\nEpoch 9/10\n422/422 - 2s - 4ms/step - accuracy: 0.9403 - loss: 0.2798 - val_accuracy: 0.8915 - val_loss: 0.4123\nEpoch 10/10\n422/422 - 2s - 4ms/step - accuracy: 0.9418 - loss: 0.2664 - val_accuracy: 0.9047 - val_loss: 0.3812\nTest Accuracy: 0.9029\nEpoch 1/10\n422/422 - 7s - 16ms/step - accuracy: 0.7324 - loss: 1.1166 - val_accuracy: 0.8588 - val_loss: 0.3752\nEpoch 2/10\n422/422 - 2s - 4ms/step - accuracy: 0.8305 - loss: 0.4741 - val_accuracy: 0.8735 - val_loss: 0.3337\nEpoch 3/10\n422/422 - 2s - 4ms/step - accuracy: 0.8561 - loss: 0.4026 - val_accuracy: 0.8877 - val_loss: 0.3007\nEpoch 4/10\n422/422 - 2s - 4ms/step - accuracy: 0.8664 - loss: 0.3656 - val_accuracy: 0.8883 - val_loss: 0.2916\nEpoch 5/10\n422/422 - 2s - 4ms/step - accuracy: 0.8771 - loss: 0.3370 - val_accuracy: 0.8953 - val_loss: 0.2806\nEpoch 6/10\n422/422 - 2s - 4ms/step - accuracy: 0.8869 - loss: 0.3102 - val_accuracy: 0.8982 - val_loss: 0.2777\nEpoch 7/10\n422/422 - 2s - 4ms/step - accuracy: 0.8904 - loss: 0.2953 - val_accuracy: 0.9032 - val_loss: 0.2755\nEpoch 8/10\n422/422 - 2s - 4ms/step - accuracy: 0.8976 - loss: 0.2838 - val_accuracy: 0.8885 - val_loss: 0.3075\nEpoch 9/10\n422/422 - 2s - 4ms/step - accuracy: 0.9006 - loss: 0.2700 - val_accuracy: 0.9065 - val_loss: 0.2595\nEpoch 10/10\n422/422 - 2s - 4ms/step - accuracy: 0.9046 - loss: 0.2538 - val_accuracy: 0.9135 - val_loss: 0.2467\nTest Accuracy: 0.9065\nEpoch 1/10\n422/422 - 8s - 18ms/step - accuracy: 0.7442 - loss: 1.2703 - val_accuracy: 0.8593 - val_loss: 0.6098\nEpoch 2/10\n422/422 - 2s - 4ms/step - accuracy: 0.8424 - loss: 0.6477 - val_accuracy: 0.8790 - val_loss: 0.5162\nEpoch 3/10\n422/422 - 2s - 4ms/step - accuracy: 0.8632 - loss: 0.5645 - val_accuracy: 0.8888 - val_loss: 0.4769\nEpoch 4/10\n422/422 - 2s - 4ms/step - accuracy: 0.8728 - loss: 0.5109 - val_accuracy: 0.8980 - val_loss: 0.4354\nEpoch 5/10\n422/422 - 2s - 4ms/step - accuracy: 0.8828 - loss: 0.4730 - val_accuracy: 0.9002 - val_loss: 0.4104\nEpoch 6/10\n422/422 - 2s - 4ms/step - accuracy: 0.8891 - loss: 0.4443 - val_accuracy: 0.9073 - val_loss: 0.3895\nEpoch 7/10\n422/422 - 2s - 4ms/step - accuracy: 0.8931 - loss: 0.4211 - val_accuracy: 0.9067 - val_loss: 0.3750\nEpoch 8/10\n422/422 - 2s - 4ms/step - accuracy: 0.8960 - loss: 0.4054 - val_accuracy: 0.9043 - val_loss: 0.3721\nEpoch 9/10\n422/422 - 2s - 4ms/step - accuracy: 0.9017 - loss: 0.3882 - val_accuracy: 0.9123 - val_loss: 0.3601\nEpoch 10/10\n422/422 - 2s - 4ms/step - accuracy: 0.9021 - loss: 0.3825 - val_accuracy: 0.9108 - val_loss: 0.3658\nTest Accuracy: 0.8981\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"### step 5: Notes \n* Regularization -> Discourages large weights (simpler model)\n* Dropout -> randomly disables neurons during training -> prevents co-adaptation\n* L1 vs L2:\n  - L1 -> Promotes sparsity(some weights become zero)\n  - L2 -> weight decay, more stable.\n* Best practice -> L2 + Dropout usually performs storngest on Fashion-MNISt as per the results","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}